{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Implementing SVMs and Kernel Methods using scikit‐learn and custom code."
      ],
      "metadata": {
        "id": "OADQAYcHhtRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVMs) use different kernels to solve various types of problems by transforming data into higher dimensions where separation becomes easier. Each kernel has its own strengths and is suited for different scenarios. Here’s a simple breakdown of the most common kernels:\n",
        "\n",
        "The Linear Kernel is the simplest and works by drawing straight lines or flat planes to separate data. Imagine trying to divide different colored marbles on a table with a ruler—this is what the linear kernel does. It’s fast and efficient for datasets where classes can be cleanly separated with straight boundaries, such as text classification or linearly separable data. However, it struggles with more complex, curved patterns.\n",
        "\n",
        "For more flexibility, the Polynomial Kernel creates curved decision boundaries using polynomial functions. Think of it like using bendy straws instead of straight rulers to separate marbles—it can twist and turn to fit moderately complex data. You can control its flexibility with the degree parameter: a higher degree allows more complex curves, but too high may cause overfitting. This kernel is useful when data requires gentle curves but isn’t overly intricate.\n",
        "\n",
        "The RBF (Radial Basis Function) Kernel, also called the Gaussian Kernel, is the most versatile and widely used. It creates smooth, flexible boundaries that can adapt to almost any shape, much like a lasso rope that can form perfect loops around groups of marbles. It works exceptionally well for highly complex datasets, such as image recognition or handwriting classification. The gamma parameter controls how tight or loose these boundaries are—small gamma gives broader curves, while large gamma fits closer to individual points.\n",
        "\n",
        "Lastly, the Sigmoid Kernel mimics the behavior of neural networks, acting like a stretchy rubber sheet to separate data. While interesting theoretically, it’s rarely used in practice today because actual neural networks tend to perform better for similar tasks. It’s mostly kept for historical reasons or specific edge cases."
      ],
      "metadata": {
        "id": "8v5iHfkHkR-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Implementating Support Vector Machines (SVMs) with Scikit-Learn"
      ],
      "metadata": {
        "id": "5lNjUDMl7dzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Standard SVC (Classification)\n",
        "The basic Support Vector Classifier (SVC) separates data using different kernel functions ('linear', 'poly', 'rbf', 'sigmoid') to transform input space. The polynomial kernel creates curved boundaries while RBF fits complex patterns. All kernels aim to maximize the margin between classes while allowing some misclassifications."
      ],
      "metadata": {
        "id": "hGD9Gk2Ll58m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTo_Wqze5kQ",
        "outputId": "e4a39ec7-30d9-4b2d-8684-7ed0bd50c185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel: linear  | Accuracy: 0.9778\n",
            "Kernel: poly    | Accuracy: 0.9556\n",
            "Kernel: rbf     | Accuracy: 1.0000\n",
            "Kernel: sigmoid | Accuracy: 0.8889\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries from scikit-learn\n",
        "from sklearn import datasets  # To load standard datasets\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into train/test sets\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.metrics import accuracy_score  # To evaluate model performance\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
        "import numpy as np  # Numerical computing library\n",
        "\n",
        "# Load the iris dataset\n",
        "# The dataset contains 150 samples of iris flowers with 4 features each and labels representing 3 species of iris\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Feature matrix (150 samples × 4 features)\n",
        "y = iris.target  # Target vector (150 labels)\n",
        "\n",
        "# Split the dataset into training (70%) and testing (30%) sets\n",
        "# random_state=42 ensures reproducible results\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a StandardScaler to standardize features\n",
        "# This transforms features to have mean=0 and std=1, which is important for SVM performance\n",
        "scaler = StandardScaler()\n",
        "# Fit the scaler on training data and transform training data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# Transform test data using the same scaler (don't fit on test data!)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the different kernel types we want to test\n",
        "# Kernels determine how SVMs transform the input space\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "\n",
        "# Test each kernel type\n",
        "for kernel in kernels:\n",
        "    # Create an SVM classifier with the current kernel type\n",
        "\n",
        "    # Special case for polynomial kernel where we need to specify degree\n",
        "    if kernel == 'poly':\n",
        "        # degree=3 means we're using cubic polynomial features\n",
        "        # gamma='scale' automatically sets gamma = 1/(n_features * X.var())\n",
        "        svm = SVC(kernel=kernel, degree=3, gamma='scale')  # Polynomial kernel of degree 3\n",
        "    else:\n",
        "        # For other kernels, just specify kernel type and gamma\n",
        "        svm = SVC(kernel=kernel, gamma='scale')\n",
        "\n",
        "    # Train the SVM model on the training data\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy by comparing predictions to true labels\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Print results with kernel name (left-aligned in 7 chars) and accuracy\n",
        "    print(f\"Kernel: {kernel:<7} | Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. LinearSVC\n",
        "An optimized version specifically for linear classification that's faster than SVC with linear kernel. It uses squared hinge loss by default and works better with large datasets. Unlike regular SVC, it scales linearly with sample size but only creates straight decision boundaries."
      ],
      "metadata": {
        "id": "HX4HSpWGmEut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Same data prep as above\n",
        "linear_svc = LinearSVC(C=1.0, loss='hinge', max_iter=10000, random_state=42)\n",
        "linear_svc.fit(X_train, y_train)\n",
        "print(f\"LinearSVC accuracy: {accuracy_score(y_test, linear_svc.predict(X_test)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiUCElkLh9Lb",
        "outputId": "edf856b5-2e30-4760-8f7e-8cb121ce0a08"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC accuracy: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Nu-SVC\n",
        "A variant where the 'nu' parameter controls the number of support vectors (between 0-1) instead of using C. Easier to interpret as nu represents the upper bound on training errors. For example, nu=0.1 means at most 10% of training points can be misclassified or sit inside the margin."
      ],
      "metadata": {
        "id": "wsAD0Pz5mJrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import NuSVC\n",
        "\n",
        "nu_svc = NuSVC(kernel='rbf', nu=0.1, gamma='scale', random_state=42)\n",
        "nu_svc.fit(X_train, y_train)\n",
        "print(f\"Nu-SVC accuracy: {accuracy_score(y_test, nu_svc.predict(X_test)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2J7j8HMk5pE",
        "outputId": "bcf4162a-0925-42f1-e032-398d85a76b5f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nu-SVC accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. SVR (Regression)\n",
        "The regression version uses an epsilon-insensitive tube (ε) where errors smaller than ε aren't penalized. Still uses kernels like RBF but predicts continuous values instead of classes. The C parameter balances margin width versus points outside the tube, while epsilon controls the tube's width.\n"
      ],
      "metadata": {
        "id": "OD9kS0YrmPNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Regression target (sepal length)\n",
        "y_reg = iris.data[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data[:, 1:], y_reg, test_size=0.3)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "svr.fit(X_train, y_train)\n",
        "print(f\"SVR MSE: {mean_squared_error(y_test, svr.predict(X_test)):.4f}\")\n",
        "print(f\"SVR R-squared: {r2_score(y_test, svr.predict(X_test)):.4f}\") # Use R-squared for regression\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHLBAzXzk84O",
        "outputId": "51813399-1031-49cc-fc67-45d47174479c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR MSE: 0.1046\n",
            "SVR R-squared: 0.8386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Custom Kernel SVM\n",
        "Allows defining your own kernel function (like the RBF example) for specialized similarity measures. The kernel function takes two data matrices and returns their similarity matrix. This flexibility enables domain-specific kernels for text, graphs, or other complex data structures."
      ],
      "metadata": {
        "id": "VQh1_vNomUDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load classification data (iris dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # y contains discrete class labels (0, 1, 2)\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Scale features (critical for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# 4. Define custom RBF kernel function\n",
        "def custom_kernel(X, Y):\n",
        "    return rbf_kernel(X, Y, gamma=0.5)  # gamma=0.5 is our custom parameter\n",
        "\n",
        "# 5. Create and train SVM with custom kernel\n",
        "custom_svm = SVC(kernel=custom_kernel)\n",
        "custom_svm.fit(X_train, y_train)  # Now using proper classification labels\n",
        "\n",
        "# 6. Evaluate\n",
        "y_pred = custom_svm.predict(X_test)\n",
        "print(f\"Custom kernel accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrM9gETwk_Ni",
        "outputId": "ae9c08c0-eb2e-423b-f500-b2f098542db5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom kernel accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Custom SVM Implementation with Different Kernels\n",
        "\n",
        "This Code implements a Support Vector Machine (SVM) from scratch with three different kernel functions:\n",
        "1. Linear Kernel\n",
        "2. Polynomial Kernel\n",
        "3. RBF (Gaussian) Kernel\n",
        "\n",
        "We'll test these on the Iris dataset for binary classification."
      ],
      "metadata": {
        "id": "U6_DREaA_GKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "QDLLR8yy_Y5I"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom SVM Class Implementation\n",
        "\n",
        "The following class implements our custom SVM with:\n",
        "- Three kernel functions (linear, polynomial, RBF) :\n",
        "   \n",
        "  The kernel functions compute the similarity between two input vectors.\n",
        "  Different kernels allow the SVM to learn different types of decision boundaries.\n",
        "- Sequential Minimal Optimization (SMO) training algorithm:\n",
        "\n",
        "  The fit method trains the SVM using Sequential Minimal Optimization (SMO), which optimizes the Lagrange multipliers (α) in pairs.\n",
        "- Prediction and evaluation methods\n",
        "  These methods make predictions using the trained SVM model."
      ],
      "metadata": {
        "id": "W6zKv9os_cDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSVM:\n",
        "    def __init__(self, kernel='linear', C=1.0, gamma=1.0, degree=3, coef0=1.0):\n",
        "        \"\"\"\n",
        "        Initialize the Support Vector Machine classifier with specified parameters.\n",
        "\n",
        "        Parameters:\n",
        "        kernel : str\n",
        "            Type of kernel function ('linear', 'poly', 'rbf')\n",
        "        C : float\n",
        "            Regularization parameter (smaller values allow more margin violations)\n",
        "        gamma : float\n",
        "            Kernel coefficient for 'rbf' and 'poly' kernels\n",
        "        degree : int\n",
        "            Degree of polynomial kernel\n",
        "        coef0 : float\n",
        "            Independent term in polynomial kernel\n",
        "        \"\"\"\n",
        "        # Store kernel type and parameters as instance variables\n",
        "        self.kernel = kernel  # Type of kernel to use\n",
        "        self.C = C            # Regularization parameter\n",
        "        self.gamma = gamma    # Kernel coefficient\n",
        "        self.degree = degree  # Polynomial degree\n",
        "        self.coef0 = coef0    # Polynomial independent term\n",
        "\n",
        "        # Initialize variables that will be set during training\n",
        "        self.alpha = None     # Lagrange multipliers (dual coefficients)\n",
        "        self.b = 0            # Bias term (intercept)\n",
        "        self.X_train = None   # Training data features\n",
        "        self.y_train = None   # Training data labels\n",
        "        self.support_vectors = None  # Support vectors after training\n",
        "\n",
        "    def _kernel(self, X1, X2):\n",
        "        \"\"\"\n",
        "        Compute the kernel matrix between two sets of samples.\n",
        "\n",
        "        Parameters:\n",
        "        X1 : array-like, shape (n_samples1, n_features)\n",
        "            First set of samples\n",
        "        X2 : array-like, shape (n_samples2, n_features)\n",
        "            Second set of samples\n",
        "\n",
        "        Returns:\n",
        "        K : array, shape (n_samples1, n_samples2)\n",
        "            Kernel matrix\n",
        "        \"\"\"\n",
        "        if self.kernel == 'linear':\n",
        "            # Linear kernel: simple dot product between samples\n",
        "            return X1 @ X2.T  # Matrix multiplication\n",
        "\n",
        "        elif self.kernel == 'poly':\n",
        "            # Polynomial kernel: (gamma*<x1,x2> + coef0)^degree\n",
        "            return (self.gamma * (X1 @ X2.T) + self.coef0) ** self.degree\n",
        "\n",
        "        elif self.kernel == 'rbf':\n",
        "            # RBF (Gaussian) kernel: exp(-gamma * ||x1-x2||^2)\n",
        "            # Compute squared Euclidean distances efficiently\n",
        "            dists = np.sum(X1**2, axis=1)[:, np.newaxis] + np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n",
        "            return np.exp(-self.gamma * dists)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unknown kernel type\")\n",
        "\n",
        "    def fit(self, X, y, max_iter=1000, tol=1e-3):\n",
        "        \"\"\"\n",
        "        Train the SVM model using Sequential Minimal Optimization (SMO) algorithm.\n",
        "\n",
        "        Parameters:\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Training vectors\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Target values (will be converted to -1/1)\n",
        "        max_iter : int\n",
        "            Maximum number of iterations\n",
        "        tol : float\n",
        "            Tolerance for stopping criterion\n",
        "        \"\"\"\n",
        "        # Convert labels to -1/1 (assuming binary classification)\n",
        "        y = y.copy()  # Avoid modifying original array\n",
        "        y[y == 0] = -1  # Convert class 0 to -1, others to 1\n",
        "\n",
        "        # Store training data and shape information\n",
        "        n_samples = X.shape[0]  # Number of training samples\n",
        "        self.X_train = X        # Store training features\n",
        "        self.y_train = y        # Store training labels\n",
        "\n",
        "        # Initialize Lagrange multipliers (alpha) to zeros\n",
        "        self.alpha = np.zeros(n_samples)\n",
        "        # Initialize bias term to zero\n",
        "        self.b = 0.0\n",
        "\n",
        "        # Precompute the kernel matrix (Gram matrix) for all training samples\n",
        "        K = self._kernel(X, X)\n",
        "\n",
        "        # Sequential Minimal Optimization (SMO) algorithm\n",
        "        for _ in range(max_iter):\n",
        "            num_changed = 0  # Track number of alpha pairs changed\n",
        "\n",
        "            for i in range(n_samples):  # Iterate through all samples\n",
        "                # Calculate prediction error for sample i\n",
        "                Ei = (self.alpha * y) @ K[:, i] + self.b - y[i]\n",
        "\n",
        "                # Check KKT conditions (violation means we should optimize this alpha_i)\n",
        "                if ((y[i] * Ei < -tol) and (self.alpha[i] < self.C)) or \\\n",
        "                   ((y[i] * Ei > tol) and (self.alpha[i] > 0)):\n",
        "\n",
        "                    # Randomly select a different sample j to optimize together\n",
        "                    j = i\n",
        "                    while j == i:  # Ensure j is different from i\n",
        "                        j = np.random.randint(0, n_samples)\n",
        "\n",
        "                    # Calculate error for sample j\n",
        "                    Ej = (self.alpha * y) @ K[:, j] + self.b - y[j]\n",
        "\n",
        "                    # Save old alpha values before updating\n",
        "                    alpha_i_old, alpha_j_old = self.alpha[i], self.alpha[j]\n",
        "\n",
        "                    # Compute L and H bounds for alpha_j\n",
        "                    if y[i] != y[j]:\n",
        "                        L = max(0, self.alpha[j] - self.alpha[i])\n",
        "                        H = min(self.C, self.C + self.alpha[j] - self.alpha[i])\n",
        "                    else:\n",
        "                        L = max(0, self.alpha[i] + self.alpha[j] - self.C)\n",
        "                        H = min(self.C, self.alpha[i] + self.alpha[j])\n",
        "\n",
        "                    if L == H:  # Skip if bounds are equal\n",
        "                        continue\n",
        "\n",
        "                    # Compute eta (second derivative of objective function)\n",
        "                    eta = 2 * K[i, j] - K[i, i] - K[j, j]\n",
        "                    if eta >= 0:  # Skip if not making progress\n",
        "                        continue\n",
        "\n",
        "                    # Update alpha_j (clipped to stay within bounds)\n",
        "                    self.alpha[j] = alpha_j_old - y[j] * (Ei - Ej) / eta\n",
        "                    self.alpha[j] = np.clip(self.alpha[j], L, H)\n",
        "\n",
        "                    # Skip if change is too small\n",
        "                    if abs(self.alpha[j] - alpha_j_old) < 1e-5:\n",
        "                        continue\n",
        "\n",
        "                    # Update alpha_i using the same amount but opposite direction\n",
        "                    self.alpha[i] = alpha_i_old + y[i] * y[j] * (alpha_j_old - self.alpha[j])\n",
        "\n",
        "                    # Update bias term b using both samples\n",
        "                    b1 = self.b - Ei - y[i] * (self.alpha[i] - alpha_i_old) * K[i, i] - \\\n",
        "                         y[j] * (self.alpha[j] - alpha_j_old) * K[i, j]\n",
        "                    b2 = self.b - Ej - y[i] * (self.alpha[i] - alpha_i_old) * K[i, j] - \\\n",
        "                         y[j] * (self.alpha[j] - alpha_j_old) * K[j, j]\n",
        "\n",
        "                    # Choose b based on where alpha_i and alpha_j are in their bounds\n",
        "                    if 0 < self.alpha[i] < self.C:\n",
        "                        self.b = b1\n",
        "                    elif 0 < self.alpha[j] < self.C:\n",
        "                        self.b = b2\n",
        "                    else:\n",
        "                        self.b = (b1 + b2) / 2\n",
        "\n",
        "                    num_changed += 1  # Increment changed counter\n",
        "\n",
        "            # If no alphas were changed in this iteration, we've converged\n",
        "            if num_changed == 0:\n",
        "                break\n",
        "\n",
        "        # After optimization, identify support vectors (alpha > 0)\n",
        "        sv_indices = self.alpha > 1e-5  # Small threshold to account for numerical precision\n",
        "        self.support_vectors = X[sv_indices]  # Store support vectors\n",
        "        self.alpha = self.alpha[sv_indices]   # Keep only alphas for support vectors\n",
        "        self.y_train = y[sv_indices]          # Keep only labels for support vectors\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        Calculate signed distance of samples to the decision boundary.\n",
        "\n",
        "        Parameters:\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Input samples\n",
        "\n",
        "        Returns:\n",
        "        array, shape (n_samples,)\n",
        "            Decision function values (signed distances)\n",
        "        \"\"\"\n",
        "        # Compute kernel matrix between input and support vectors\n",
        "        K = self._kernel(X, self.support_vectors)\n",
        "        # Calculate decision values using support vector alphas and labels\n",
        "        return (self.alpha * self.y_train) @ K.T + self.b\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Input samples\n",
        "\n",
        "        Returns:\n",
        "        array, shape (n_samples,)\n",
        "            Predicted class labels (-1 or 1)\n",
        "        \"\"\"\n",
        "        # Convert decision function values to class labels\n",
        "        return np.sign(self.decision_function(X))"
      ],
      "metadata": {
        "id": "5qmkAbF8_mBW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on Iris Dataset\n",
        "\n",
        "Now we'll test our custom SVM with different kernels on the Iris dataset."
      ],
      "metadata": {
        "id": "SM9W2PXIBmTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset from scikit-learn\n",
        "# The Iris dataset contains 150 samples with 4 features each (sepal length/width, petal length/width)\n",
        "# and 3 classes of iris flowers (setosa=0, versicolor=1, virginica=2)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # X contains features, y contains class labels\n",
        "\n",
        "# Prepare for binary classification by:\n",
        "# 1. Removing class 2 (virginica) to create a 2-class problem\n",
        "# 2. Converting remaining classes (0 and 1) to -1 and 1 respectively\n",
        "# This creates a binary classification problem between setosa (-1) and versicolor (1)\n",
        "X = X[y != 2]  # Keep only samples where class is not 2\n",
        "y = y[y != 2]  # Similarly filter the labels\n",
        "y = np.where(y == 0, -1, 1)  # Convert class 0 to -1, class 1 to 1\n",
        "\n",
        "# Split the dataset into training and testing sets:\n",
        "# - 70% for training (X_train, y_train)\n",
        "# - 30% for testing (X_test, y_test)\n",
        "# random_state=42 ensures reproducible results across runs\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling is crucial for SVMs because:\n",
        "# 1. Features on different scales can dominate the kernel calculations\n",
        "# 2. It helps the optimization converge faster\n",
        "scaler = StandardScaler()  # Creates a scaler that standardizes features (mean=0, std=1)\n",
        "X_train = scaler.fit_transform(X_train)  # Fit scaler to training data and transform\n",
        "X_test = scaler.transform(X_test)  # Transform test data using same scaling parameters\n",
        "\n",
        "# Define the kernels to test:\n",
        "# 1. Linear kernel (no additional parameters needed)\n",
        "# 2. Polynomial kernel with degree=3 and gamma=0.1\n",
        "# 3. RBF kernel with gamma=0.1\n",
        "kernels = [\n",
        "    ('linear', {}),  # Simple linear kernel\n",
        "    ('poly', {'degree': 3, 'gamma': 0.1}),  # 3rd degree polynomial kernel\n",
        "    ('rbf', {'gamma': 0.1}),  # Radial Basis Function (Gaussian) kernel\n",
        "]\n",
        "\n",
        "print(\"Improved Iris Dataset Classification Results:\")\n",
        "print(\"-----------------------------------\")\n",
        "\n",
        "# Test each kernel configuration\n",
        "for kernel, params in kernels:\n",
        "    # Initialize SVM with current kernel and parameters\n",
        "    # C=1.0 is the regularization parameter (balance between margin and classification error)\n",
        "    svm = CustomSVM(kernel=kernel, C=1.0, **params)\n",
        "\n",
        "    # Train the SVM model on the training data\n",
        "    # This runs the SMO algorithm to find the optimal decision boundary\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy by comparing predictions to true labels\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Print results for this kernel:\n",
        "    print(f\"{kernel.upper()} Kernel:\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")  # Classification accuracy on test set\n",
        "    print(f\"  Number of support vectors: {len(svm.support_vectors)}\")  # Points defining the boundary\n",
        "    print(f\"  Bias term: {svm.b:.4f}\")  # Offset of the decision boundary\n",
        "    print(\"-----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0eHk_42lF_M",
        "outputId": "81b8590d-3f86-495b-df81-257911871a9f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved Iris Dataset Classification Results:\n",
            "-----------------------------------\n",
            "LINEAR Kernel:\n",
            "  Accuracy: 1.0000\n",
            "  Number of support vectors: 8\n",
            "  Bias term: 0.9467\n",
            "-----------------------------------\n",
            "POLY Kernel:\n",
            "  Accuracy: 1.0000\n",
            "  Number of support vectors: 12\n",
            "  Bias term: 0.0298\n",
            "-----------------------------------\n",
            "RBF Kernel:\n",
            "  Accuracy: 1.0000\n",
            "  Number of support vectors: 13\n",
            "  Bias term: 0.1133\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zm76SL5LMxXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
